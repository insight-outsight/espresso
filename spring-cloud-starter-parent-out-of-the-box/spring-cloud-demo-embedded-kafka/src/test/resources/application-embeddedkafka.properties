kafka.producer.bootstrap.servers = localhost:9692
kafka.producer.client.id = producerTest1112Demo
kafka.producer.acks = 1
kafka.producer.retries = 3
#批处理条数,当多个记录被发送至统一分区时，producer对于同一个分区来说，会按照 batch.size 的大小进行统一收集，批量发送
kafka.producer.batch.size = 4096
#与 batch.size 配合使用。延迟统一收集，产生聚合，然后批量发送至broker
kafka.producer.linger.ms = 10
# 33554432 即32MB的批处理缓冲区
#kafka.producer.buffer.memory = 40960
 
#默认 topic
#kafka.producer.defaultTopic = testTopic11123
kafka.producer.key.serializer = org.apache.kafka.common.serialization.StringSerializer
kafka.producer.value.serializer = org.apache.kafka.common.serialization.StringSerializer
 
#消费端 brokers 集群
kafka.consumer.bootstrap.servers = localhost:9692
#消费者 group.id 组ID
kafka.consumer.group.id = test-group-11123
#消费者消费消息后，进行自动提交
kafka.consumer.enable.auto.commit = true
#自动提交的频率(与 enable.auto.commit = true 属性配合使用)
kafka.consumer.auto.commit.interval.ms = 1000
#新的groupid,是否从头开始消费
kafka.consumer.auto.offset.reset = earliest
#在使用kafka组管理时，发送心跳机制，用于检测消费者故障的超时
#kafka.consumer.session.timeout.ms = 1000
 
kafka.consumer.key.deserializer = org.apache.kafka.common.serialization.StringDeserializer
kafka.consumer.value.deserializer = org.apache.kafka.common.serialization.StringDeserializer
 
#消费端消费的topic
#kafka.consumer.topic = testTopic11123